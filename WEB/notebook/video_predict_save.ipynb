{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch\n",
    "# !pip3 install pretrainedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from os.path import join\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_default_data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
    "    x1 = face.left()\n",
    "    y1 = face.top()\n",
    "    x2 = face.right()\n",
    "    y2 = face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "    if minsize:\n",
    "        if size_bb < minsize:\n",
    "            size_bb = minsize\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "    # Check for out of bounds, x-y top left corner\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    return x1, y1, size_bb\n",
    "\n",
    "\n",
    "def preprocess_image(image, cuda=True):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    preprocess = xception_default_data_transforms['test']\n",
    "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    if cuda:\n",
    "        preprocessed_image = preprocessed_image.cuda()\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),\n",
    "                       cuda=True):\n",
    "    # Preprocess\n",
    "    preprocessed_image = preprocess_image(image, cuda)\n",
    "\n",
    "    # Model prediction\n",
    "    output = model(preprocessed_image)\n",
    "    output = post_function(output)\n",
    "\n",
    "    # Cast to desired\n",
    "    _, prediction = torch.max(output, 1)    # argmax\n",
    "    prediction = float(prediction.cpu().numpy())\n",
    "\n",
    "    return int(prediction), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_image_network(video_path, model, output_path,\n",
    "                            start_frame=0, end_frame=None, cuda=True):\n",
    "    \"\"\"\n",
    "    Reads a video and evaluates a subset of frames with the a detection network\n",
    "    that takes in a full frame. Outputs are only given if a face is present\n",
    "    and the face is highlighted using dlib.\n",
    "    :param video_path: path to video file\n",
    "    :param model_path: path to model file (should expect the full sized image)\n",
    "    :param output_path: path where the output video is stored\n",
    "    :param start_frame: first frame to evaluate\n",
    "    :param end_frame: last frame to evaluate\n",
    "    :param cuda: enable cuda\n",
    "    :return:\n",
    "    \n",
    "    # Modified to take in the model file instead of model\n",
    "    \"\"\"\n",
    "    #print('Starting: {}'.format(video_path))\n",
    "    \n",
    "    predictions = []\n",
    "    outputs = []\n",
    "\n",
    "    # Read and write\n",
    "    reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    writer = None\n",
    "\n",
    "    # Face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Load model\n",
    "#     model, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
    "#     if model_path is not None:\n",
    "#         model = torch.load(model_path)\n",
    "#         print('Model found in {}'.format(model_path))\n",
    "#     else:\n",
    "#         print('No model found, initializing random model.')\n",
    "#     if cuda:\n",
    "#         model = model.cuda()\n",
    "\n",
    "    # Text variables\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    thickness = 2\n",
    "    font_scale = 1\n",
    "\n",
    "    # Frame numbers and length of output video\n",
    "    frame_num = 0\n",
    "    assert start_frame < num_frames - 1\n",
    "    end_frame = end_frame if end_frame else num_frames\n",
    "    pbar = tqdm(total=end_frame-start_frame)\n",
    "\n",
    "    while reader.isOpened():\n",
    "        _, image = reader.read()\n",
    "        if image is None:\n",
    "            break\n",
    "        frame_num += 1\n",
    "\n",
    "        if frame_num < start_frame:\n",
    "            continue\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Image size\n",
    "#         print('getting image size')\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Init output writer\n",
    "#         print('init output writer')\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,\n",
    "                                     (height, width)[::-1])\n",
    "\n",
    "        # 2. Detect with dlib\n",
    "#         print('detect with dlib')\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector(gray, 1)\n",
    "        if len(faces):\n",
    "            # For now only take biggest face\n",
    "            face = faces[0]\n",
    "\n",
    "            # --- Prediction ---------------------------------------------------\n",
    "            # Face crop with dlib and bounding box scale enlargement\n",
    "            x, y, size = get_boundingbox(face, width, height)\n",
    "            cropped_face = image[y:y+size, x:x+size]\n",
    "\n",
    "            # Actual prediction using our model\n",
    "            prediction, output = predict_with_model(cropped_face, model,\n",
    "                                                    cuda=cuda)\n",
    "            predictions.append(prediction)\n",
    "            outputs.append(output)\n",
    "            # ------------------------------------------------------------------\n",
    "\n",
    "            # Text and bb\n",
    "            x = face.left()\n",
    "            y = face.top()\n",
    "            w = face.right() - x\n",
    "            h = face.bottom() - y\n",
    "            label = 'fake' if prediction == 1 else 'real'\n",
    "            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
    "            output_list = ['{0:.2f}'.format(float(x)) for x in\n",
    "                           output.detach().cpu().numpy()[0]]\n",
    "            cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n",
    "                        font_face, font_scale,\n",
    "                        color, thickness, 2)\n",
    "            # draw box over face\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        if frame_num >= end_frame:\n",
    "            break\n",
    "\n",
    "        # Show\n",
    "#         print('show result')\n",
    "        # cv2.imshow('test', image)\n",
    "#         cv2.waitKey(33)     # About 30 fps\n",
    "        writer.write(image)\n",
    "    pbar.close()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        #print('Finished! Output saved under {}'.format(output_path))\n",
    "    else:\n",
    "        pass\n",
    "        #print('Input video file was empty')\n",
    "    return predictions, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
    "\n",
    "# def predict_model(video_fn, model,\n",
    "#                   start_frame=0, end_frame=30,\n",
    "#                   plot_every_x_frames = 5):\n",
    "#     \"\"\"\n",
    "#     Given a video and model, starting frame and end frame.\n",
    "#     Predict on all frames.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     fn = video_fn.split('.')[0]\n",
    "#     label = metadata.loc[video_fn]['label']\n",
    "#     original = metadata.loc[video_fn]['original']\n",
    "#     video_path = f'../input/deepfake-detection-challenge/train_sample_videos/{video_fn}'\n",
    "#     output_path = './'\n",
    "#     test_full_image_network(video_path, model, output_path, start_frame=0, end_frame=30, cuda=False)\n",
    "#     # Read output\n",
    "#     vidcap = cv2.VideoCapture(f'{fn}.avi')\n",
    "#     success,image = vidcap.read()\n",
    "#     count = 0\n",
    "#     fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "#     axes = axes = axes.flatten()\n",
    "#     i = 0\n",
    "#     while success:\n",
    "#         # Show every xth frame\n",
    "#         if count % plot_every_x_frames == 0:\n",
    "\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             axes[i].imshow(image)\n",
    "#             axes[i].set_title(f'{fn} - frame {count} - true label: {label}')\n",
    "#             axes[i].xaxis.set_visible(False)\n",
    "#             axes[i].yaxis.set_visible(False)\n",
    "#             i += 1\n",
    "#         success,image = vidcap.read()\n",
    "#         count += 1\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def video_file_frame_pred(video_path, model,\n",
    "#                           start_frame=0, end_frame=300,\n",
    "#                           cuda=True, n_frames=5):\n",
    "#     \"\"\"\n",
    "#     Predict and give result as numpy array\n",
    "#     \"\"\"\n",
    "#     pred_frames = [int(round(x)) for x in np.linspace(start_frame, end_frame, n_frames)]\n",
    "#     predictions = []\n",
    "#     outputs = []\n",
    "#     # print('Starting: {}'.format(video_path))\n",
    "\n",
    "#     # Read and write\n",
    "#     video_path = f'../input/deepfake-detection-challenge/train_sample_videos/{video_path}'\n",
    "#     reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "#     video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "#     fps = reader.get(cv2.CAP_PROP_FPS)\n",
    "#     num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     writer = None\n",
    "\n",
    "#     # Face detector\n",
    "#     face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "#     # Text variables\n",
    "#     font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#     thickness = 2\n",
    "#     font_scale = 1\n",
    "\n",
    "#     # Frame numbers and length of output video\n",
    "#     frame_num = 0\n",
    "#     assert start_frame < num_frames - 1\n",
    "#     end_frame = end_frame if end_frame else num_frames\n",
    "#     while reader.isOpened():\n",
    "#         _, image = reader.read()\n",
    "#         if image is None:\n",
    "#             break\n",
    "#         frame_num += 1\n",
    "#         if frame_num in pred_frames:\n",
    "#             height, width = image.shape[:2]\n",
    "#             # 2. Detect with dlib\n",
    "#             gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             faces = face_detector(gray, 1)\n",
    "#             if len(faces):\n",
    "#                 # For now only take biggest face\n",
    "#                 face = faces[0]\n",
    "#                 # --- Prediction ---------------------------------------------------\n",
    "#                 # Face crop with dlib and bounding box scale enlargement\n",
    "#                 x, y, size = get_boundingbox(face, width, height)\n",
    "#                 cropped_face = image[y:y+size, x:x+size]\n",
    "\n",
    "#                 # Actual prediction using our model\n",
    "#                 prediction, output = predict_with_model(cropped_face, model,\n",
    "#                                                         cuda=cuda)\n",
    "#                 predictions.append(prediction)\n",
    "#                 outputs.append(output)\n",
    "#                 # ------------------------------------------------------------------\n",
    "#         if frame_num >= end_frame:\n",
    "#             break\n",
    "#     # Figure out how to do this with torch\n",
    "#     preds_np = [x.detach().cpu().numpy()[0][1] for x in outputs]\n",
    "#     if len(preds_np) == 0:\n",
    "#         return predictions, outputs, 0.5, 0.5, 0.5\n",
    "#     try:\n",
    "#         mean_pred = np.mean(preds_np)\n",
    "#     except:\n",
    "#         # couldnt find faces\n",
    "#         mean_pred = 0.5\n",
    "#     min_pred = np.min(preds_np)\n",
    "#     max_pred = np.max(preds_np)\n",
    "#     return predictions, outputs, mean_pred, min_pred, max_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'network.models.TransferModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model_path = './all_raw.p'\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "# model = torch.load(model_path)\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict_model('aettqgevhz.mp4', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_file_frame_pred('aettqgevhz.mp4', model, n_frames=4, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred('aettqgevhz.mp4', model, n_frames=4, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [tensor([[9.9931e-01, 6.8923e-04]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[9.9930e-01, 6.9542e-04]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[9.9988e-01, 1.1859e-04]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[9.9999e-01, 1.2059e-05]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[9.9999e-01, 8.9372e-06]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[1.0000e+00, 1.7430e-06]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[1.0000e+00, 6.8130e-07]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[1.0000e+00, 2.3482e-07]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[1.0000e+00, 4.9894e-07]], grad_fn=<SoftmaxBackward>),\n",
       "  tensor([[1.0000e+00, 2.2484e-06]], grad_fn=<SoftmaxBackward>)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full_image_network(f'../input/deepfake-detection-challenge/train_sample_videos/aettqgevhz.mp4', model, './', start_frame=0, end_frame=10, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
